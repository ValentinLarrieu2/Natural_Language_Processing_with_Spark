{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We import the data\n",
    "dfraw = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \";\"). \\\n",
    "         csv(\"/FileStore/tables/powerDataForTP.csv\") \n",
    "\n",
    "df = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \";\"). \\\n",
    "         csv(\"/FileStore/tables/powerDataForTP.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "####We compute the mean of column and replace the missing value with it\n",
    "mean_dict = { col: 'mean' for col in df.columns }\n",
    "col_avgs = df.agg( mean_dict ).collect()[0].asDict()\n",
    "col_avgs = { k[4:-1]: v for k,v in col_avgs.iteritems() }\n",
    "dfmodified=df.fillna( col_avgs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(dfmodified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "vec= VectorAssembler(\n",
    "  inputCols= [\n",
    "  'AT',\n",
    "  'V',\n",
    "  'AP',\n",
    "  'RH'\n",
    "  ],\n",
    "  outputCol = 'features'\n",
    ")\n",
    "data= vec.transform(dfmodified)\n",
    "print(data)\n",
    "modelData = data.select('features','PE')\n",
    "trainData, testData = modelData.randomSplit([0.7,0.3])\n",
    "\n",
    "#Visualization of the variables\n",
    "modelData.describe().show()\n",
    "trainData.describe().show()\n",
    "testData.describe().show()\n",
    "\n",
    "#Model Creation\n",
    "lr= LinearRegression(labelCol='PE',featuresCol='features',regParam=0.01)\n",
    "\n",
    "#We fit the model\n",
    "lrModel=lr.fit(trainData)\n",
    "\n",
    "#we create a summary and display it\n",
    "summary=lrModel.summary\n",
    "summary.predictions.show(truncate=False)\n",
    "\n",
    "\n",
    "#Evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "#Estimator\n",
    "lr1= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "lr2= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "lr3= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid1 = ParamGridBuilder().\\\n",
    "    addGrid(lr1.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr1.elasticNetParam, [0]).\\\n",
    "    build()\n",
    "param_grid2 = ParamGridBuilder().\\\n",
    "    addGrid(lr2.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr2.elasticNetParam, [1]).\\\n",
    "    build()\n",
    "param_grid3 = ParamGridBuilder().\\\n",
    "    addGrid(lr2.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr2.elasticNetParam, [0.2, 0.4,0.6, 0.8]).\\\n",
    "    build()\n",
    "# cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv1 = CrossValidator(estimator=lr1, estimatorParamMaps=param_grid1, evaluator=evaluator, numFolds=4)\n",
    "cv2 = CrossValidator(estimator=lr2, estimatorParamMaps=param_grid2, evaluator=evaluator, numFolds=4)\n",
    "cv3 = CrossValidator(estimator=lr3, estimatorParamMaps=param_grid3, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "#Fit cross-validation model\n",
    "cv_model1 = cv1.fit(trainData)\n",
    "cv_model2 = cv2.fit(trainData)\n",
    "cv_model3 = cv3.fit(trainData)\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv1 = cv_model1.transform(trainData)\n",
    "pred_test_cv1 = cv_model1.transform(testData)\n",
    "\n",
    "pred_training_cv2 = cv_model2.transform(trainData)\n",
    "pred_test_cv2 = cv_model2.transform(testData)\n",
    "\n",
    "pred_training_cv3 = cv_model3.transform(trainData)\n",
    "pred_test_cv3 = cv_model3.transform(testData)\n",
    "\n",
    "#Evaluation\n",
    "# performance on training data\n",
    "print(\"Train data performance Lasso = \", evaluator.evaluate(pred_training_cv1))\n",
    "print(\"Train data performance Ridge = \", evaluator.evaluate(pred_training_cv2))\n",
    "print(\"Train data performance Elastic = \", evaluator.evaluate(pred_training_cv3))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Lasso = \", evaluator.evaluate(pred_test_cv1))\n",
    "print(\"Test data performance Ridge = \", evaluator.evaluate(pred_test_cv2))\n",
    "print(\"Test data performance Elastic = \", evaluator.evaluate(pred_test_cv3))\n",
    "\n",
    "#Intercept and coefficients\n",
    "print(\"Best intercept Lasso = \", cv_model1.bestModel.intercept)\n",
    "#print(\"Best coefficients Lasso = \", cv_model1.bestModel.coefficients)\n",
    "\n",
    "print(\"Best intercept Ridge = \", cv_model2.bestModel.intercept)\n",
    "#print(\"Best coefficients Ridge = \", cv_model2.bestModel.coefficients)\n",
    "\n",
    "print(\"Best intercept Elastic = \", cv_model3.bestModel.intercept)\n",
    "#print(\"Best coefficients Elastic = \", cv_model3.bestModel.coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,RandomForestRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "trainData2, testData2 = dfmodified.randomSplit([0.7,0.3])\n",
    "\n",
    "dt = DecisionTreeRegressor(maxDepth=3)\n",
    "rf = RandomForestRegressor(numTrees=3, maxDepth=3)\n",
    "\n",
    "dt.setLabelCol(\"PE\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "rf.setLabelCol(\"PE\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setFeaturesCol(\"features\")\n",
    "  \n",
    "#rf = RandomForestRegressor(labelCol=\"PE\", featuresCol=\"features\", predictionCol(\"prediction\"))\n",
    "\n",
    "# We create the Pipeline\n",
    "dtPipeline = Pipeline()\n",
    "rfPipeline = Pipeline()\n",
    "\n",
    "# We set the parameters of the pipeline\n",
    "dtPipeline.setStages([vec, dt])\n",
    "rfPipeline.setStages([vec, rf])\n",
    "\n",
    "eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"PE\", metricName=\"rmse\")\n",
    "crossval = CrossValidator(estimator=dtPipeline, evaluator=eval, numFolds=4)\n",
    "crossval2 = CrossValidator(estimator=rfPipeline, evaluator=eval, numFolds=4)\n",
    "crossval.setEstimator(dtPipeline)\n",
    "crossval2.setEstimator(rfPipeline)\n",
    "\n",
    "# We try different maxDepth\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1,2,3,4,5])\n",
    "             .build())\n",
    "\n",
    "paramGrid2 = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxBins, [20,40,60,80,100])\n",
    "             .build())\n",
    "\n",
    "# We add the grid\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "crossval2.setEstimatorParamMaps(paramGrid2)\n",
    "\n",
    "# We generate the best model\n",
    "dtModel = crossval.fit(trainData2).bestModel\n",
    "rfModel = crossval2.fit(trainData2).bestModel\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv3 = dtModel.transform(trainData2)\n",
    "pred_test_cv3 = dtModel.transform(testData2)\n",
    "\n",
    "pred_training_cv4 = rfModel.transform(trainData2)\n",
    "pred_test_cv4 = rfModel.transform(testData2)\n",
    "\n",
    "# performance on training data\n",
    "print(\"Train data performance Decision Tree = \", evaluator.evaluate(pred_training_cv3))\n",
    "print(\"Train data performance Random Forest = \", evaluator.evaluate(pred_training_cv4))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Decision Tree = \", evaluator.evaluate(pred_test_cv3))\n",
    "print(\"Test data performance Random Forest = \", evaluator.evaluate(pred_test_cv4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################PART2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,RandomForestRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "\n",
    "#We read the file\n",
    "rdd=sc.textFile(\"/FileStore/tables/SMSSpamCollection.txt\").map(lambda line: line.split('\\t')) \n",
    "rddMap = rdd.map(lambda scmr:(1 if scmr[0]==\"spam\" else 0,scmr[1]))\n",
    "#We convert it to dataframe\n",
    "dfSpam = rddMap.toDF([\"label\",\"message\"])#we create the dataframe\n",
    "\n",
    "#We only select word of length>3\n",
    "regex_tokenizer = RegexTokenizer(inputCol='message', outputCol='words', pattern='\\\\W').setMinTokenLength(3)\n",
    "regex_df = regex_tokenizer.transform(dfSpam)\n",
    "regex_df.show(truncate=False)\n",
    "\n",
    "#We create 2 and 3 gram\n",
    "ngram2 = NGram(n=2, inputCol='words', outputCol='2grams')\n",
    "gramdf2 = ngram2.transform(regex_df)\n",
    "ngram3 = NGram(n=3, inputCol='words', outputCol='3grams')\n",
    "gramdf3 =ngram3.transform(gramdf2)\n",
    "#We count the words\n",
    "count_words = udf(lambda words: len(words), IntegerType())\n",
    "messageTreatedDf=gramdf3.withColumn('counts', count_words('words'))\n",
    "messageTreatedDf.show()\n",
    "\n",
    "#We create the therm frequency of the 3 columns words, 2gram and 3grams\n",
    "tf1 = HashingTF(inputCol='words', outputCol='wordsf')\n",
    "tf_df1 = tf1.transform(messageTreatedDf)\n",
    "tf2 = HashingTF(inputCol='2grams', outputCol='2gramsf')\n",
    "tf_df2 = tf2.transform(tf_df1)\n",
    "tf3 = HashingTF(inputCol='3grams', outputCol='3gramsf')\n",
    "tf_df3 = tf3.transform(tf_df2)\n",
    "#We create the coresponding IDF\n",
    "idf1 = IDF(inputCol='wordsf', outputCol='features')\n",
    "idf2 = IDF(inputCol='2gramsf', outputCol='2gramsfIDF')\n",
    "idf3 = IDF(inputCol='3gramsf', outputCol='3gramsfIDF')\n",
    "idf_model1 = idf1.fit(tf_df3)\n",
    "dataIDF1=idf_model1.transform(tf_df3)\n",
    "idf_model2 = idf2.fit(dataIDF1)\n",
    "dataIDF2=idf_model2.transform(dataIDF1)\n",
    "idf_model3 = idf3.fit(dataIDF2)\n",
    "dataIDF3=idf_model3.transform(dataIDF2)\n",
    "#display(dataIDF3)\n",
    "\n",
    "vec= VectorAssembler(\n",
    "  inputCols= [\n",
    "  'wordsfIDF'\n",
    "  ],\n",
    "  outputCol = 'features'\n",
    ")\n",
    "\n",
    "#data= vec.transform(dataIDF3)\n",
    "#display(data)\n",
    "modelData = dataIDF3.select('label','features')\n",
    "trainData3, testData3 = modelData.randomSplit([0.7,0.3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataS = dataIDF3.select('label','words')\n",
    "count_vec = CountVectorizer(inputCol='words', outputCol='features',  minDF=1)\n",
    "modelC = count_vec.fit(dataS)\n",
    "data2 = modelC.transform(dataS)\n",
    "modelData2 = data2.select('label','features')\n",
    "trainData4, testData4 = modelData2.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "#Model Creation\n",
    "lr= LinearRegression(labelCol='label',featuresCol='features',regParam=0.01)\n",
    "\n",
    "#We fit the model\n",
    "lrModel=lr.fit(trainData3)\n",
    "\n",
    "#we create a summary and display it\n",
    "summary=lrModel.summary\n",
    "summary.predictions.show(truncate=False)\n",
    "\n",
    "testResults = lrModel.evaluate(testData3)\n",
    "\n",
    "testResults.residuals.show(n=10) #diff y true & y pref = accuracy\n",
    "testResults.residuals.groupBy().avg().show()\n",
    "pred=testResults.predictions\n",
    "df=testResults.residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "#Model Creation\n",
    "lr= LogisticRegression(labelCol='label',featuresCol='features',regParam=0.01)\n",
    "\n",
    "#We fit the model\n",
    "lrModel=lr.fit(trainData4)\n",
    "\n",
    "#We set our evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# performance on training data\n",
    "pred_training=lrModel.transform(trainData4)\n",
    "print(\"Train data performance Logistic regression = \", evaluator.evaluate(pred_training))\n",
    "\n",
    "# performance on test data\n",
    "pred_test=lrModel.transform(testData4)\n",
    "print(\"Test data performance Logistic regression = \", evaluator.evaluate(pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision tree\n",
    "dt = DecisionTreeClassifier(labelCol='label',featuresCol='features', predictionCol=\"prediction\",maxDepth=30)\n",
    "\n",
    "#We fit our model\n",
    "dtModel=dt.fit(trainData4)\n",
    "\n",
    "#We set our evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# performance on training data\n",
    "pred_training=dtModel.transform(trainData4)\n",
    "print(\"Train data performance Decision Tree = \", evaluator.evaluate(pred_training))\n",
    "\n",
    "# performance on test data\n",
    "pred_test=dtModel.transform(testData4)\n",
    "print(\"Test data performance Decision Tree = \", evaluator.evaluate(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random forest\n",
    "rf = RandomForestClassifier(labelCol='label',featuresCol='features', predictionCol=\"prediction\", numTrees=1, maxDepth=30)\n",
    "\n",
    "#We fit our model\n",
    "rfModel=rf.fit(trainData4)\n",
    "\n",
    "#We set our evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# performance on training data\n",
    "pred_training=rfModel.transform(trainData4)\n",
    "print(\"Train data performance Logistic regression = \", evaluator.evaluate(pred_training))\n",
    "\n",
    "# performance on test data\n",
    "pred_test=rfModel.transform(testData4)\n",
    "print(\"Test data performance Logistic regression = \", evaluator.evaluate(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive bayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "nb.setLabelCol(\"label\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "#We fit our model\n",
    "nbModel=nb.fit(trainData4)\n",
    "\n",
    "#testResultsnb = nbModel.transform(testData3)\n",
    "\n",
    "#We set our evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# performance on training data\n",
    "pred_training=nbModel.transform(trainData4)\n",
    "print(\"Train data performance Logistic regression = \", evaluator.evaluate(pred_training))\n",
    "\n",
    "# performance on test data\n",
    "pred_test=nbModel.transform(testData4)\n",
    "print(\"Test data performance Logistic regression = \", evaluator.evaluate(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model optimisation: cross validation on set of TF-IDF\n",
    "\n",
    "#Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "#Estimator\n",
    "lr1= LogisticRegression(labelCol='label',featuresCol='features',regParam=0.01)\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid1 = ParamGridBuilder().\\\n",
    "    addGrid(lr1.regParam, [0.0001,0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]).\\\n",
    "    build()\n",
    "\n",
    "    \n",
    "# cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv1 = CrossValidator(estimator=lr1, estimatorParamMaps=param_grid1, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "#Fit cross-validation model\n",
    "cv_model1 = cv1.fit(trainData3)\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv1 = cv_model1.transform(trainData3)\n",
    "pred_test_cv1 = cv_model1.transform(testData3)\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "# performance on training data\n",
    "print(\"Train data performance Logistic regression = \", evaluator.evaluate(pred_training_cv1))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Logistic regression = \", evaluator.evaluate(pred_test_cv1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model optimisation: cross validation on set of Count Vectoriser\n",
    "\n",
    "#Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "#Estimator\n",
    "lr1= LogisticRegression(labelCol='label',featuresCol='features',regParam=0.01)\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid1 = ParamGridBuilder().\\\n",
    "    addGrid(lr1.regParam, [0.0001,0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]).\\\n",
    "    build()\n",
    "\n",
    "    \n",
    "# cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv1 = CrossValidator(estimator=lr1, estimatorParamMaps=param_grid1, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "#Fit cross-validation model\n",
    "cv_model1 = cv1.fit(trainData4)\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv1 = cv_model1.transform(trainData4)\n",
    "pred_test_cv1 = cv_model1.transform(testData4)\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "# performance on training data\n",
    "print(\"Train data performance Logistic regression = \", evaluator.evaluate(pred_training_cv1))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Logistic regression = \", evaluator.evaluate(pred_test_cv1))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "TP4_Rendu_Grincourt_Larrieu",
  "notebookId": 3.740699848577986E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
